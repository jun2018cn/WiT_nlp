{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. spaCy basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython.The library is published under the MIT license and currently offers statistical neural network models for English, German, Spanish, Portuguese, French, Italian, Dutch and multi-language NER, as well as tokenization for various other languages.$^{1}$\n",
    "\n",
    "https://spacy.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and setup \n",
    "\n",
    "1. First we need to download the spaCy library:\n",
    "    > `conda install -c conda-forge spacy`\n",
    "    > <br>*or*<br>\n",
    "    > `pip install -U spacy`\n",
    "2. Then, we download the language model we want to use:\n",
    "    > `python -m spacy download en_core_web_lg`\n",
    "    > <br>*alternatively*<br>\n",
    "    > `import spacy.cli` <br>\n",
    "    >  `spacy.cli.download(\"en_core_web_lg\") `\n",
    "3. Finally, you load the model:\n",
    "    > `spacy.load('en_core_web_lg')`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# load the language model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Doc object\n",
    "doc = nlp(u\"The quick brown fox jumps over the lazy dog!\")\n",
    "\n",
    "# Print each token separately\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a description for a given POS tag, dependency label or entity type\n",
    "\n",
    "spacy.explain('PUNCT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Pipeline\n",
    "When we call `nlp` on our string, we are creating a `DOC` object which is \n",
    "basically an array of token objects made from that string. We can then perform a series of operations on that tokens in order to dercribe the data.   <br>Image source: https://spacy.io/usage/spacy-101#pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/pipeline1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "This is usually the first step in Natural Language Processing. Tokenization means 'chopping' the text into pieces in order to create tokens. Since we are using spaCy with a pretrained language model, these tokens also contain some descriptive information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic tokenization with a split() function\n",
    "text = \"While Amazon CEO Jeff Bezos wasn't throwing a lavish party at his $23 million mansion in Washington, DC\"\n",
    "print(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a full list of `Part Of Speech` Tags visit https://spacy.io/api/annotation#pos-tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how `wasn't` has been split into two tokens. spaCy recognizes both the root verb is and the negation attached to it. Notice also that both the extended whitespace and the period at the end of the sentence are assigned their own tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens are pieces of the original text. That is, we don't see any conversion to word stems or lemmas (base forms of words) and we haven't seen anything about organizations/places/money etc. Tokens are the basic building blocks of a Doc object - everything that helps us understand the meaning of the text is derived from tokens and their relationship to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Dependencies\n",
    "We also see the syntactic dependencies assigned to each token. `Bezos` is identified as an `nsubj` or the ***nominal subject*** of the sentence.\n",
    "\n",
    "For a full list of Syntactic Dependencies visit https://spacy.io/api/annotation#dependency-parsing\n",
    "<br>A good explanation of dependencies can be found [here](https://nlp.stanford.edu/software/dependencies_manual.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('nsubj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other token attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Tag|Description|\n",
    "|:------|:------:|\n",
    "|`.text`|The original word text|\n",
    "|`.lemma_`|The base form of the word|\n",
    "|`.pos_`|The simple part-of-speech tag|\n",
    "|`.tag_`|The detailed part-of-speech tag|\n",
    "|`.shape_`|The word shape – capitalization, punctuation, digits|\n",
    "|`.is_alpha`|Is the token an alpha character?|\n",
    "|`.is_stop`|Is the token part of a stop list, i.e. the most common words of the language?|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "NER introduces another layer of context. The spaCy language model recognizes that certain tokens represent for example locations or organizations. \n",
    "https://spacy.io/usage/linguistic-features#named-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = nlp(u'Apple to build a Hong Kong factory for $6 million')\n",
    "\n",
    "for token in ner:\n",
    "    print(token.text, end=' | ')\n",
    "\n",
    "print('\\n----')\n",
    "\n",
    "for ent in ner.ents:\n",
    "    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAJA: We'll also see how spaCy can interpret the tokens combined `$23 million` as referring to ***money***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'By 2010 standards, the iPad was thin—half an inch thin—and weighed 1.5 pounds. It had a capacitive multitouch display, ran on Apple’s custom A4 chip, and offered 10 hours of battery life, something that would win it praise in early reviews. WIRED’s review noted that watching video on it was “terrific,” as was reading on it, and that the iPad was well-positioned as a gaming platform. But the iPad was also hamstrung in its earliest incarnation. It didn’t have a camera, it didn’t support multitasking, the Safari browsing experience was limited, and the virtual keyboard came with a learning curve')\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing dependency parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Apple is going to build a U.K. factory for $6 million.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the arches of dependecy parser do not appear on the visualization, in a new cell run `displacy.serve(doc, style='dep')` and then open new browser page and type: `http://127.0.0.1:5000/`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization and stemming\n",
    "\n",
    "`Stemming` is a proces of reducing inflected words to their word stem. Here, \"boat\" would be the stem for [boat, boater, boating, boats]. However, in languages with many exceptions we need more sophisticated methods to perform proper stemming. spaCy does not include a stemmer, inbsted it relies entirely on the lemmatization.\n",
    "<br><br>\n",
    "`Lemmatization` on the other hand applies morphological analysis to the word and considers full vocabulary of the language. As a result, the lemma of `was` is be, `better` is good and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"Running was the only thing he could think of while playing with knives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(f'{token.text:{12}} {token.lemma_:{12}} {token.pos_:{8}} {spacy.explain(token.tag_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "Certain words appear so often in language that they do not require tagging. These words are called `stop words` and we usually filter them out from the processed text. spaCy and other NLP libraries contain inbuilt lists of stop words that are ready for usage. NB stop words list can be modified. spaCy contains 326 stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(nlp.Defaults.stop_words)\n",
    "#len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a word is a stop word\n",
    "nlp.vocab['me'].is_stop\n",
    "\n",
    "# add a stop word\n",
    "nlp.Defaults.stop_words.add('macaroni')\n",
    "\n",
    "# set the stop_word tag on the lexeme\n",
    "nlp.vocab['macaroni'].is_stop = True\n",
    "\n",
    "# remove the word from the list\n",
    "nlp.Defaults.stop_words.remove('macaroni')\n",
    "\n",
    "# remove the stop_word tag from the lexeme\n",
    "nlp.vocab['macaroni'].is_stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: <br/>\n",
    "1.Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word embeddings \n",
    "\n",
    "In a nutshell word embeddings or word vectors are numerical representation of text. Embeddings are capable of capturing the context of a word within a document and provide a broader meaning.\n",
    "\n",
    "`You shall know a word by the company it keeps` said by a famous British linguist John Rupert Firth represent this idea, meaning that the words in part determined by its collocations. In that way, if we represent words in a multidimensional space the words similar to each like `car` and `bike` other will have closer values. Cosine similarity is used to calculate the distance between the vectors - and it is a measure how similar the words are (0:1)\n",
    "\n",
    "Mostly used models: Word2Vec (SkipGram and CBOW algorithms) and GloVe.\n",
    "\n",
    "Before feeding the text to any machine learning model we first need to convert text strings into numbers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word embeddings with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the large english language model - that was trained on OntoNotes - large corpus comprising various genres \n",
    "# of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows). \n",
    "# With GloVe vectors trained on CommonCrawl - 685k unique vectors (300 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acces vector a word\n",
    "nlp('car').vector\n",
    "\n",
    "# shape o a word vector\n",
    "nlp('car').vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp('car').similarity(nlp('vehicle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from: https://www.kaggle.com/snap/amazon-fine-food-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = #TODO : read the Reviews.csv dataset into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Text'][200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: drop NaN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: create a subset of dataset in order to improve performance - subset of 1000-10000 reviews\n",
    "subset ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import defaultdict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning function for removing stopwords and creating lemma\n",
    "# TODO: fill in the ..... in order to remove stopwords and create lemma\n",
    "def cleaning(doc):\n",
    "    txt = [ ..... for ..... in ..... if not .....]\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a 'dirty' review before processing\n",
    "subset[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase the words and remove all html tags and store the results in remove_html_tags\n",
    "\n",
    "#TODO: add the lowercase function to review row\n",
    "remove_html_tags = [re.sub(re.compile('<.*?>'), '', str(row))...... for row in .....]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results after removing html tags\n",
    "remove_html_tags[200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results you should receive: <br>\n",
    "`\"even with small containers, they don't fill them up.  these little tins are less than half filled and at the price charged it seems a rip-off. is there some exotic ingredient as costly as gold contained in those tiny squares?  or how about the cereal ploy, they were filled at the factory but settled in transport.can manufacturers be honest in their dealings?\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all non-alphabetic characters and store in remove_non_alpha\n",
    "\n",
    "#TODO:remove all non-alphabetic characters using regular expression\n",
    "remove_non_alpha = [re.sub(....., ' ', row) for row in .....]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results after removing non-alphabetic characters\n",
    "remove_non_alpha[200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results you should get: <br>\n",
    "`\"even with small containers they don't fill them up these little tins are less than half filled and at the price charged it seems a rip off is there some exotic ingredient as costly as gold contained in those tiny squares or how about the cereal ploy they were filled at the factory but settled in transport can manufacturers be honest in their dealings \"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: apply the cleaning function defined previously\n",
    "txt = [..... for doc in nlp.pipe(remove_non_alpha, batch_size = 1000, n_threads = -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original text\n",
    "reviews['Text'][200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned text\n",
    "print(txt[200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install gensim via anaconda `conda install -c conda-forge gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For full documentation go to: <br> `https://radimrehurek.com/gensim/models/ldamodel.html`<br> `https://radimrehurek.com/gensim/corpora/dictionary.html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dictionary, which is a mapping of word IDs to words.\n",
    "id2word = corpora.Dictionary(txt)\n",
    "\n",
    "# Turns each document into a bag of words - containing how many times each word (word id) occured within a document\n",
    "corpus = [id2word.doc2bow(doc) for doc in txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id2word[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words : occurences or a word within a document. BoW disregards all informatio about the word meaning or word order. It simply counts how many occurences of a word is there in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=42,\n",
    "                                           #update_every=1,\n",
    "                                           #passes=10,\n",
    "                                           alpha=.1, #distribution of the number of topics per document\n",
    "                                           chunksize=100,\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coherence score give an approximation of how good the topics are, it determines the optimal number of topics.\n",
    "# So, the higher the score the better : 0 < x < 1\n",
    "\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=txt, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=txt, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyLDAvis import gensim as pyLDAvis_gensim\n",
    "import pickle \n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization below, will allow you to compare topics and see what is the distribution of words per topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vis = pyLDAvis_gensim.prepare(lda_model, corpus, id2word, sort_topics=True)\n",
    "#lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
